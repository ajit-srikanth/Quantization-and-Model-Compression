(base) skills@phoenix:~$ python main.py                                                                                                                     model size before quantization : 5362.966624 MB                                                                                                              62%|███████████████████████████████████████████████████████████████████████▌                                            | 926/1500 [11:45<07:34,  1.26i 62%|███████████████████████████████████████████████████████████████████████▋                                            | 927/1500 [11:46<08:02,  1.19i 62%|███████████████████████████████████████████████████████████████████████▊                                            | 928/1500 [11:47<08:20,  1.14i 62%|███████████████████████████████████████████████████████████████████████▊                                            | 929/1500 [11:48<07:41,  1.24i 62%|███████████████████████████████████████████████████████████████████████▉                                            | 930/1500 [11:49<08:05,  1.17i 62%|███████████████████████████████████████████████████████████████████████▉                                            | 931/1500 [11:50<08:22,  1.13i 62%|████████████████████████████████████████████████████████████████████████                                            | 932/1500 [11:50<07:16,  1.30i 62%|████████████████████████████████████████████████████████████████████████▏                                           | 934/1500 [11:51<06:03,  1.56i 62%|███████████████████████████ 75%|█████████████████████████████████████████████████████████████████████████████████████▋                             | 1118/1500 [14:03<02:58,  2.14it/s] 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 1365/1500 [17:00<02:03,  1.09it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [18:44<00:00,  1.33it/s]Overall Average Perplexity before quantization: 11.6426                                                                                                     GPTNeoForCausalLM(                                                                                                                                            (transformer): GPTNeoModel(                                                                                                                                   (wte): Embedding(50257, 2048)                                                                                                                               (wpe): Embedding(2048, 2048)                                                                                                                                (drop): Dropout(p=0.0, inplace=False)                                                                                                                       (h): ModuleList(                                                                                                                                              (0-23): 24 x GPTNeoBlock(                                                                                                                                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)                                                                                              (attn): GPTNeoAttention(                                                                                                                                      (attention): GPTNeoSelfAttention(                                                                                                                             (attn_dropout): Dropout(p=0.0, inplace=False)                                                                                                               (resid_dropout): Dropout(p=0.0, inplace=False)                                                                                                              (k_proj): Linear(in_features=2048, out_features=2048, bias=False)                                                                                           (v_proj): Linear(in_features=2048, out_features=2048, bias=False)                                                                                           (q_proj): Linear(in_features=2048, out_features=2048, bias=False)                                                                                           (out_proj): Linear(in_features=2048, out_features=2048, bias=True)                                                                                        )                                                                                                                                                         )                                                                                                                                                           (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)                                                                                       

        (mlp): GPTNeoMLP(                                                                                                                            [15/50]          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)                                                                                              (c_proj): Linear(in_features=8192, out_features=2048, bias=True)                                                                                            (act): NewGELUActivation()                                                                                                                                  (dropout): Dropout(p=0.0, inplace=False)                                                                                                                  )                                                                                                                                                         )                                                                                                                                                         )                                                                                                                                                           (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)                                                                                            )                                                                                                                                                           (lm_head): Linear(in_features=2048, out_features=50257, bias=False)                                                                                       )                                                                                                                                                           GPTNeoForCausalLM(                                                                                                                                            (transformer): GPTNeoModel(                                                                                                                                   (wte): Embedding(50257, 2048)                                                                                                                               (wpe): Embedding(2048, 2048)                                                                                                                                (drop): Dropout(p=0.0, inplace=False)                                                                                                                       (h): ModuleList(                                                                                                                                              (0-23): 24 x GPTNeoBlock(                                                                                                                                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)                                                                                              (attn): GPTNeoAttention(                                                                                                                                      (attention): GPTNeoSelfAttention(                                                                                                                             (attn_dropout): Dropout(p=0.0, inplace=False)                                                                                                               (resid_dropout): Dropout(p=0.0, inplace=False)                                                                                                              (k_proj): W8A16LinearLayer()                                                                                                                                (v_proj): W8A16LinearLayer()                                                                                                                                (q_proj): W8A16LinearLayer()                                                                                                                                (out_proj): W8A16LinearLayer()                                                                                                                            )                                                                                                                                                         )                                                                                                                                                           (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)                                                                                              (mlp): GPTNeoMLP(                                                                                                                                             (c_fc): W8A16LinearLayer()                                                                                                                                  (c_proj): W8A16LinearLayer()                                                                                                                                (act): NewGELUActivation()                                                                                                                                  (dropout): Dropout(p=0.0, inplace=False)                                                                                                                  )                                             
)


      )                                                                                                                                                         )                                                                                                                                                           (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)                                                                                            )                                                                                                                                                           (lm_head): Linear(in_features=2048, out_features=50257, bias=False)                                                                                       )                                                                                                                                                           model size after quantization : 1740.86 MB                                                                                                                  Memory saved : 3622.11 MB                                                                                                                                    23%|██████████████████████████▉                                                                                         | 349/1500 [04:57<14:22,  1.33it/s] 23%|███████████████████████████                                                                                         | 350/1500 [04:58<15:50,  1.21it/s] 25%|█████████████████████████████▎                                                                                      | 379/1500 [05:24<14:20,  1.30it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [19:57<00:00,  1.25it/s]Overall Average Perplexity after quantization: 11.6493        